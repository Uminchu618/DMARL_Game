# AGENTS.md

このリポジトリで作業する **AI コードエージェント向けガイドライン** です。

このリポジトリは、**Python による強化学習 (RL) や世界モデル／生成モデルの研究** を目的としています。  
あなたの役割は次のとおりです。

- RL および関連モデルのアルゴリズムを実装・拡張すること
- 実験を再現可能にするためのテストとツールを整備すること

---

## 1. 技術スタックと制約

- 言語: **Python 3.12**
- 代表的なライブラリ（必要に応じて利用）:
  - 数値計算・配列: `numpy`
  - 機械学習・深層学習: `torch` など
  - 可視化: `matplotlib`
- パッケージ・環境管理:
  - **uv** を使用すること（`pip` や `venv` を直接使う構成には寄せない）

## 2. 目標とするリポジトリ構成

以下を「理想的な構成」とみなしてください。足りないファイルは、必要に応じて作成して構いません。

```text
.
├── AGENTS.md               # このファイル
├── README.md               # 人間向け概要
├── pyproject.toml          # uv 用プロジェクト設定
├── uv.lock                 # uv のロックファイル（存在する場合）
├── paper/tex                   # 論文のtexファイル
├── src/
│   └── rl_lab/
│       ├── __init__.py
│       ├── envs.py            # 環境定義および環境ラッパ
│       ├── agents.py          # エージェント実装
│       ├── training.py        # 学習ループ（トレーニング）
│       ├── evaluation.py      # 評価・指標計算
│       ├── config.py          # 設定オブジェクトとローダ
│       └── utils.py           # 共通ユーティリティ関数
├── notebooks/
│   └── *.ipynb                # 探索・可視化用ノートブック
└── tests/
    ├── test_envs.py
    ├── test_agents.py
    ├── test_training.py
    ├── test_evaluation.py
    └── conftest.py
```

ルール:

- **ノートブックは「可視化・インタラクティブ解析」が主目的**。  
  重い学習や大量実験は基本的に CLI 実行を前提にする。
- 実装コードは必ず `src/rl_lab/**` 配下に置くこと。


---

## 3. セットアップと実行

すべての依存関係管理・実行に **uv** を使用します。

### 3.1 uv の前提

ホスト環境に uv がインストールされていることを前提とします。  
エージェントは OS レベルのインストール操作（`apt`, `brew` など）は行いません。  
必要な場合は人間向けに README にインストール方法を追記してください。

### 3.2 依存関係のセットアップ

`pyproject.toml` が存在する場合（推奨）:

```bash
uv sync
```

`pyproject.toml` がまだ整備されていない場合、一時的に `requirements*.txt` を利用しても構いませんが、可能であれば **pyproject.toml に統合** する方向で作業してください。

暫定的なコマンド例:

```bash
uv pip install -r requirements.txt
# 開発用依存があれば
uv pip install -r requirements-dev.txt
```

### 3.3 テストの実行

テストには **pytest** を使用します。実行は必ず `uv run` 経由で行ってください。

```bash
uv run pytest

# 高速に回したい場合の一例
uv run pytest -m "not slow"
```

重いテストや時間のかかるテストを追加する場合は、適宜 `@pytest.mark.slow` を使って切り分けてください。

### 3.4 学習・評価スクリプトの実行例（CLI）

CLI は、**コンフィグを差し替えながらの実験** を行うための標準的な入り口とします。

```bash
# 学習
uv run python -m rl_lab.training \
    --config configs/experiment1.yaml

# 評価
uv run python -m rl_lab.evaluation \
    --config configs/experiment1_eval.yaml
```

- CLI の実装には `argparse` または `typer` を用いて構いません。
- 実験条件（環境名、ハイパーパラメータ、学習ステップ数など）は、コード中にハードコードせず `configs/` 以下の設定ファイルに切り出してください。
- 「同じコードでコンフィグだけ変えて比較する」ことを前提に設計すること。

### 3.5 CLI と Notebook の役割

CLI と Notebook で役割が違います。以下を守ってください。

- **CLI (コマンドライン実行)**
  - 目的: コンフィグを変えながら大量に実験する、本番に近い学習・評価を回す。
  - 特徴:
    - wandb ログ・モデル保存・シード設定など、実験に必要な処理を一通りここで行う。
    - 再現性・比較可能性を優先し、インタラクティブな操作は基本前提としない。

- **Notebook**
  - 目的: 可視化・結果のインタラクティブな分析・デバッグ。
  - 特徴:
    - すでに学習済みの結果やログ（wandb の run、保存済みモデルなど）を読み込み、グラフやテーブルで可視化する。
    - 必要であれば、`rl_lab.training` の一部を小さいステップ数で呼び出して挙動を確認してもよいが、「本番学習」は CLI 側に寄せる。
    - Notebook 内に新しい中核的ロジック（環境・エージェント・トレーニング処理など）を定義せず、必ず `src/rl_lab/**` を import して使う。

---

## 4. コードスタイルと品質

### 4.1 スタイル

- **PEP 8** に準拠したスタイルを採用してください。
- 公開関数・クラスには **型ヒント** を付けること。
- Docstring 形式は Google スタイルまたは NumPy スタイルのいずれかを採用し、ファイル内では統一してください。
- 読みやすさと明快さを優先し、過度にトリッキーなワンライナーは避けること。

### 4.2 ruff による統一

コードのフォーマットと Lint は **ruff に統一** します。

- フォーマット: `ruff format`
- Lint: `ruff check`

### 4.3 Docstring とコメント

人間がコードの意図を素早く理解できることを最重視します。  
クラス・関数の Docstring には、最低限以下を含めてください。

- 関数・メソッドの役割の一文要約
- **引数 (Args)**: 各引数の意味・型・スケール・単位（必要なら）
- **返り値 (Returns)**: 返り値の型・形・意味
- 例外や前提条件がある場合は、それも明記

特に、学習ループまわりや環境・エージェントのインタフェースなど、抽象度が高い箇所については、Docstring を薄くしないこと。

#### PyTorch テンソル引数の記述ルール

PyTorch のテンソル／ベクトルを引数や返り値として扱う場合、Docstring で **次元と意味を具体的に** 記述してください。

例（Google スタイル）:

```python
def forward(self, x: torch.Tensor) -> torch.Tensor:
    """特徴量ベクトルからアクションのロジットを計算する。

    Args:
        x: 観測特徴量のテンソル。
            形状は (batch_size, obs_dim)。
            - batch_size: バッチサイズ
            - obs_dim: 観測の特徴次元

    Returns:
        torch.Tensor: アクションロジットのテンソル。
            形状は (batch_size, action_dim)。
            - action_dim: 離散アクションの数
    """
```

例（別の次元構造のケース）:

- 時系列: `(batch_size, time_steps, feature_dim)`
- 画像: `(batch_size, channels, height, width)`

など、**各次元の役割を列挙して説明** してください。  

---

## 5. テストポリシー

新しいコードを追加・リファクタリングするときは、必ず対応するテストを用意してください。  
テストは基本的に `tests/` ディレクトリ配下に配置します。

### 5.1 モデル・エージェントのテスト

- 代表的なモデルやエージェントについて、**小さな入力** を用いたスモークテストを用意してください。
  - 例: バッチサイズを小さくした入力で `forward` / `act` / `update` を呼び出し、例外が発生しないことを確認する。
  - 出力の **形状** や **型** が想定通りであることを確認する。
  - loss 値や報酬推定値などが NaN や Inf になっていないことを確認する。
- 乱数に依存する処理については、`torch.manual_seed` や `numpy.random.seed` などでシードを固定し、テストを安定させてください。

### 5.2 環境のテスト

- `reset` および `step` が、設計した仕様どおりの型・shape のオブジェクト（観測・報酬・終了フラグ・補助情報など）を返すことを確認してください。
- 必要に応じて、固定シード下で同じ初期状態が再現されるかといった、**最小限の再現性** を確認するテストを追加して構いません。

### 5.3 トレーニング・評価コードのテスト

- `training.py`:
  - 簡略化した設定・ダミーのモデル／エージェント・環境を使い、短いステップ数だけ学習を回すスモークテストを用意してください。
  - その際、例外が発生せず、学習ループが最後まで到達することを確認します。

- `evaluation.py`:
  - 簡易的なモデル／エージェント・環境を用いて評価関数を実行し、指標が期待される型（例: `dict[str, float]`）で返ってくることを確認してください。

### 5.4 一般ルール

- 新しいモジュールを追加した場合は、対応するテストファイルを 1 つ以上追加すること。
- 実行時間の長いテストは可能な限り簡略化し、それでも長い場合は `@pytest.mark.slow` を付けて切り分けてください。

---

## 6. ロギングとメトリクス

ロギングおよびメトリクスの収集には **Weights & Biases (wandb)** を使用します。

- wandb 関連の初期化やログ出力は、専用のヘルパ関数／クラス（例: `utils.py` 内）にまとめ、コードのあちこちで直接 `wandb.init`, `wandb.log` を呼び出さないようにしてください。
- 機密情報や不要な大量ログを送信しないよう、ログ内容には注意してください。
- 実験名・タグ・設定などのメタデータは、wandb の run 単位でわかりやすいよう整理してください。
- Notebook から wandb を使う場合も、できる限り同じヘルパを経由して呼ぶようにし、CLI とログ形式を揃えてください。

## 8. 不確実なときの方針

判断に迷う場合の優先順位は次のとおりです。

1. まず **小さく局所的で巻き戻しやすい変更** を選ぶ。
2. 明確な改善理由がない限り、既存の命名規則・構造に合わせる。
3. 新しいパターン（例: コンフィグの書き方やロギングの流儀）を導入する場合は、リポジトリ全体の一貫性を損なわないよう注意する。
1. コードベースを **一貫してモジュール的かつ理解しやすい構造** に保つこと。
2. 新しい振る舞いには必ず **テストを用意** し、`uv run pytest` で確認できる状態にすること。
3. 実験を **再現可能** かつ **反復しやすい** 形で維持すること。
